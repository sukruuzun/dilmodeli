`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.

================================================================================
QUANTIZATION COMPARISON: NASH-SWARM vs UNIFORM 4-BIT
ðŸ“Š SAMPLE SIZE: 50 (Optimized: 2.5Ã— more than baseline, memory-efficient)
================================================================================

[1/5] Loading model...
ðŸ“¦ Model: gpt2
âœ“ Model loaded: 124,439,808 parameters
âœ“ Using MPS (Metal) acceleration âš¡

[2/5] Loading WikiText-2...
âœ“ Loaded 2542 text samples

[3/5] Testing Baseline (FP32)...
  Size: 474.70 MB
  Loss: 5.4138
  Perplexity: 224.49

[4/5] Testing Uniform 4-bit...
  Theoretical Size: 18.40 MB
  Compression: 87.5%
  Loss: 6.6896
  Perplexity: 804.00

[5/5] Testing Nash-Swarm (Mixed 2/4/8-bit)...
  Theoretical Size: 13.80 MB
  Compression: 90.6%
  Avg bits/param: 3.00
  Loss: 5.4202
  Perplexity: 225.92

================================================================================
FINAL RESULTS
================================================================================

Method                         Size (MB)    Compression    Loss       Perplexity  
--------------------------------------------------------------------------------
Baseline (FP32)                    474.70             -    5.4138      224.49
Uniform 4-bit                       18.40         87.5%    6.6896      804.00
Nash-Swarm (Mixed)                  13.80         90.6%    5.4202      225.92

================================================================================
DELTA ANALYSIS (vs Baseline)
================================================================================

Method                         Î” Loss       Î” Perplexity   
--------------------------------------------------------------------------------
Uniform 4-bit                      +23.56%        +258.14%
Nash-Swarm (Mixed)                  +0.12%          +0.64%

================================================================================
KEY FINDINGS
================================================================================

1. COMPRESSION:
   âœ“ Nash-Swarm: 90.6% (HIGHER than Uniform)
   âœ“ Uniform 4-bit: 87.5%
   âœ“ Nash-Swarm uses adaptive bit-width (avg 3.00 bits/param)

2. ACCURACY:
   âœ“ Nash-Swarm loss change: +0.12% (BETTER than Uniform)
   âœ“ Uniform 4-bit loss change: +23.56%

3. THEORETICAL INSIGHT:
   Nash-Swarm allocates bits based on parameter importance:
   - Top 10% params: 8-bit (critical weights)
   - Middle 20%: 4-bit (moderate importance)
   - Bottom 70%: 2-bit (less critical)
   
   This adaptive strategy achieves HIGHER compression while
   maintaining BETTER accuracy compared to uniform quantization.

NOTE: This is a CPU-only, theoretical comparison.
Actual memory savings require hardware-specific int4/int8 support.


âœ… Test completed successfully!

