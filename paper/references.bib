% Nash Equilibrium
@article{nash1950equilibrium,
  title={Equilibrium points in n-person games},
  author={Nash, John F},
  journal={Proceedings of the National Academy of Sciences},
  volume={36},
  number={1},
  pages={48--49},
  year={1950},
  publisher={National Acad Sciences}
}

% Swarm Intelligence - Boids
@inproceedings{reynolds1987flocks,
  title={Flocks, herds and schools: A distributed behavioral model},
  author={Reynolds, Craig W},
  booktitle={Proceedings of the 14th annual conference on Computer graphics and interactive techniques},
  pages={25--34},
  year={1987}
}

% Particle Swarm Optimization
@inproceedings{kennedy1995particle,
  title={Particle swarm optimization},
  author={Kennedy, James and Eberhart, Russell},
  booktitle={Proceedings of ICNN'95-international conference on neural networks},
  volume={4},
  pages={1942--1948},
  year={1995},
  organization={IEEE}
}

% Mixture of Experts - Switch Transformer
@article{fedus2021switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={120},
  pages={1--39},
  year={2022}
}

% ST-MoE
@article{zoph2022designing,
  title={Designing effective sparse expert models},
  author={Zoph, Barret and Bello, Irwan and Kumar, Sameer and Du, Nan and Huang, Yanping and Dean, Jeff and Shazeer, Noam and Fedus, William},
  journal={arXiv preprint arXiv:2202.08906},
  year={2022}
}

% GLaM
@article{du2022glam,
  title={GLaM: Efficient Scaling of Language Models with Mixture-of-Experts},
  author={Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others},
  journal={arXiv preprint arXiv:2112.06905},
  year={2021}
}

% GPTQ Quantization
@inproceedings{frantar2023gptq,
  title={GPTQ: Accurate post-training quantization for generative pre-trained transformers},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

% AWQ
@article{lin2023awq,
  title={AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song},
  journal={arXiv preprint arXiv:2306.00978},
  year={2023}
}

% LLM.int8()
@article{dettmers2022llm,
  title={LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30318--30332},
  year={2022}
}

% Transformer
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

% GANs (Game Theory in ML)
@article{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

% Multi-agent RL
@article{lowe2017multi,
  title={Multi-agent actor-critic for mixed cooperative-competitive environments},
  author={Lowe, Ryan and Wu, Yi I and Tamar, Aviv and Harb, Jean and Pieter Abbeel, OpenAI and Mordatch, Igor},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

% PyTorch
@incollection{pytorch2019,
  title={PyTorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  booktitle={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

% MoEfication
@article{zhang2022moefication,
  title={Moefication: Transformer feed-forward layers are mixtures of experts},
  author={Zhang, Zhengyan and Lin, Yankai and Liu, Zhiyuan and Li, Peng and Sun, Maosong and Zhou, Jie},
  journal={arXiv preprint arXiv:2110.01786},
  year={2022}
}

% Starling murmuration (biological reference)
@article{ballerini2008interaction,
  title={Interaction ruling animal collective behavior depends on topological rather than metric distance: Evidence from a field study},
  author={Ballerini, Michele and Cabibbo, Nicola and Candelier, Raphael and Cavagna, Andrea and Cisbani, Evaristo and Giardina, Irene and Lecomte, Vincent and Orlandi, Alberto and Parisi, Giorgio and Procaccini, Andrea and others},
  journal={Proceedings of the national academy of sciences},
  volume={105},
  number={4},
  pages={1232--1237},
  year={2008}
}

