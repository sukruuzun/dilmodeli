# ğŸ¦… Nash-Swarm Optimization for LLMs

[![Paper](https://img.shields.io/badge/ArXiv-2024-red)](https://arxiv.org/abs/XXXX.XXXXX)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)

## ğŸ“– Overview

Nash-Swarm Optimization combines **Game Theory (Nash Equilibrium)** and **Swarm Intelligence (Starling Murmuration)** for efficient LLM compression. This framework achieves adaptive quantization with superior compression and accuracy preservation.

**Key Results:**
- âœ… **90.5-90.7% compression** across GPT-2 family (124M-1.5B)
- âœ… **196Ã— better accuracy** for small models vs uniform quantization
- âœ… **0.93Ã— inference overhead** (7% faster than baseline)
- âœ… **Model-specific behavior** validated across 4 scales

## ğŸ¯ Temel Kavramlar

### SÄ±ÄŸÄ±rcÄ±k SÃ¼rÃ¼leri (SÃ¼rÃ¼ ZekasÄ±)
- **Ä°lke**: Topluluk Uyumu ve Lokal EtkileÅŸim
- **LLM KarÅŸÄ±lÄ±ÄŸÄ±**: MoE YÃ¶nlendirme ve Kuantizasyon
- Her birey sadece en yakÄ±n komÅŸularÄ±na tepki verir, ancak tÃ¼m sÃ¼rÃ¼ uyumlu hareket eder

### Nash Dengesi (Oyun Teorisi)
- **Ä°lke**: Stabilite ve Rasyonel Karar
- **LLM KarÅŸÄ±lÄ±ÄŸÄ±**: EÄŸitim/Ä°nce Ayar KararlÄ±lÄ±ÄŸÄ±
- TÃ¼m oyuncularÄ±n stratejilerini deÄŸiÅŸtirmek iÃ§in teÅŸvikinin olmadÄ±ÄŸÄ± optimum nokta

## ğŸš€ Ã–zellikler

### 1. Nash-SÃ¼rÃ¼ MoE YÃ¶nlendirme
- Lokal uzman gruplarÄ± ile verimli routing
- CPU Ã¶nbellek optimizasyonu
- Dinamik iÅŸ yÃ¼kÃ¼ dengeleme

### 2. Dinamik Kuantizasyon ve Budama
- KomÅŸu aÄŸÄ±rlÄ±k bloklarÄ±nÄ±n etkileÅŸimi
- Nash dengesine dayalÄ± budama kararlarÄ±
- GerÃ§ek zamanlÄ± adaptasyon

### 3. Dengeleyici KayÄ±p Fonksiyonu
```
L_Nash-SÃ¼rÃ¼ = L_Ã‡aprazEntropi + Î»â‚ Â· L_Dengeleme - Î»â‚‚ Â· R_CPU-Ã–nbellek
```

## ğŸ“ Project Structure

```
dilmodeli/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/              # Core math and algorithm engine
â”‚   â”œâ”€â”€ quantization/      # Adaptive quantization logic
â”‚   â”œâ”€â”€ optimization/      # CPU optimization layer
â”‚   â””â”€â”€ visualization/     # Analysis and visualization
â”œâ”€â”€ tests/                 # Comprehensive test suite
â”‚   â”œâ”€â”€ compare_simple_quantization.py  # Main quantization tests
â”‚   â”œâ”€â”€ benchmark_speed.py              # Speed benchmarks
â”‚   â””â”€â”€ comprehensive_speed_test.py     # Multi-model validation
â”œâ”€â”€ paper/                 # Academic paper (LaTeX)
â”œâ”€â”€ docs/                  # Documentation
â””â”€â”€ requirements.txt       # Python dependencies
```

## ğŸ› ï¸ Kurulum

```bash
pip install -r requirements.txt
```

## ğŸ’¡ Quick Start

### Option 1: Local Installation (Recommended for GPT-2)

```bash
# Clone repository
git clone https://github.com/sukruuzun/dilmodeli
cd dilmodeli

# Install dependencies
pip install -r requirements.txt

# Run GPT-2 test (~5 minutes)
python tests/compare_simple_quantization.py

# Optional: Run speed benchmark
python tests/benchmark_speed.py
```

**System Requirements:**
- Python 3.8+
- 8GB RAM (GPT-2 models)
- 16GB+ RAM (larger models)
- CPU or GPU (MPS/CUDA)

### Option 2: Google Colab (No Installation!) â­

**Run in your browser with free GPU:**

ğŸ““ **Quick Demo (5 mins):** [See COLAB_SETUP.md](notebooks/COLAB_SETUP.md) for copy-paste instructions

```python
# Just 2 cells in Colab:
# 1. Setup
!git clone https://github.com/sukruuzun/dilmodeli.git && cd dilmodeli && pip install -q -r requirements.txt

# 2. Run
!python tests/compare_simple_quantization.py
```

ğŸ¦™ **LLaMA Test:** Use Colab with GPU for larger models (see [Tutorial](TUTORIAL.md))

### Option 3: Quick Test (Single Command)

```bash
# Fast test on GPT-2 with 10 samples (~2 minutes)
python -c "from tests.compare_simple_quantization import main; main()"
```

## ğŸ“Š Performance Results

### Compression (Universal)
| Model | Baseline | Uniform 4-bit | Nash-Swarm | Advantage |
|-------|----------|---------------|------------|-----------|
| GPT-2 (124M) | 475 MB | 87.5% | **90.7%** | +3.2% |
| GPT-2 Medium (355M) | 1,354 MB | 87.5% | **90.7%** | +3.2% |
| GPT-2 Large (774M) | 2,953 MB | 87.5% | **90.5%** | +3.0% |
| GPT-2 XL (1.5B) | 5,942 MB | 87.5% | **90.6%** | +3.1% |

### Accuracy (Model-Specific)
| Model | Uniform 4-bit | Nash-Swarm | Advantage |
|-------|---------------|------------|-----------|
| GPT-2 (124M) | +23.56% loss âŒ | **+0.12%** âœ… | **196Ã— better!** |
| GPT-2 Medium (355M) | +4.91% âš ï¸ | **-0.15%** âœ… | Baseline+ |
| GPT-2 Large (774M) | +1.83% | **+1.71%** âœ… | Comparable |
| GPT-2 XL (1.5B) | -0.66% | +0.41% | Both good |

### Speed (Fast!)
| Model | Baseline | Nash-Swarm | Overhead |
|-------|----------|------------|----------|
| GPT-2 (124M) | 18.24 ms | **15.46 ms** | **0.85Ã— (15% faster!)** |
| GPT-2 Medium (355M) | 28.61 ms | 28.81 ms | 1.01Ã— (comparable) |
| **Average** | - | - | **0.93Ã— (7% faster)** |

## ğŸ”¬ Research & Paper

**"Nash-Swarm Optimization: A Game-Theoretic and Bio-Inspired Framework for Large Language Model Compression"**

ğŸ“„ **Paper:** [ArXiv:XXXX.XXXXX](https://arxiv.org/abs/XXXX.XXXXX) *(Coming soon)*  
ğŸ“Š **Results:** See `FINAL_RESULTS_50_SAMPLES.md` for comprehensive analysis  
âš¡ **Speed:** See `PAPER_SPEED_UPDATE.md` for benchmark details  
ğŸ“š **Tutorial:** See [TUTORIAL.md](TUTORIAL.md) for detailed usage guide

### Key Insights
- âœ… **Compression is scale-invariant** (90.5-90.7% across all models)
- âœ… **Accuracy is model-specific** (adaptive methods crucial for small models)
- âœ… **Speed without compromise** (faster inference + better accuracy)
- âœ… **Adaptive quantization is essential** ("one-size-fits-all" fails)

### For Researchers

**Reproducing Results:**
1. ğŸ–¥ï¸ **Local:** Clone repo + run tests (requires 8-16GB RAM)
2. â˜ï¸ **Colab:** Copy-paste [COLAB_SETUP.md](notebooks/COLAB_SETUP.md) (free GPU!)
3. ğŸ“Š **Custom:** See [TUTORIAL.md](TUTORIAL.md) for advanced usage

**Testing Your Model:**
```python
from tests.compare_simple_quantization import quantize_nash_swarm_adaptive

# Apply to any Hugging Face model
quantized_model, info = quantize_nash_swarm_adaptive(your_model)
print(f"Compression: {info['compression_ratio']:.1f}%")
```

**Expected Runtime:**
- GPT-2 (124M): ~5 minutes (CPU)
- GPT-2 Medium (355M): ~10 minutes (CPU)
- GPT-2 XL (1.5B): ~40 minutes (CPU/MPS)
- LLaMA (1B+): Use Colab with GPU

### Citation
```bibtex
@article{uzun2024nashswarm,
  title={Nash-Swarm Optimization: A Game-Theoretic and Bio-Inspired Framework for Large Language Model Compression},
  author={Uzun, ÅÃ¼krÃ¼},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2024}
}
```

## ğŸ“ License

MIT License

## ğŸ¤ Contributing

Contributions welcome! Please feel free to submit a Pull Request.

## ğŸ“§ Contact

For questions and suggestions, please open an issue.

