
================================================================================
üî¨ COMPREHENSIVE SPEED ANALYSIS
================================================================================

================================================================================
Testing: gpt2
================================================================================
[1/5] Loading model...
  ‚úì Using MPS
[2/5] Measuring baseline speed...
  ‚úì Baseline: 18.24 ms (¬±0.92)
[3/5] Measuring quantization time...
  ‚úì Quantization took: 2.95 seconds (one-time)
[4/5] Measuring Nash-Swarm speed...
  ‚úì Nash-Swarm: 15.46 ms (¬±1.69)
[5/5] Analysis...

================================================================================
Testing: gpt2-medium
================================================================================
[1/5] Loading model...
  ‚úì Using MPS
[2/5] Measuring baseline speed...
  ‚úì Baseline: 28.61 ms (¬±1.11)
[3/5] Measuring quantization time...
  ‚úì Quantization took: 7.26 seconds (one-time)
[4/5] Measuring Nash-Swarm speed...
  ‚úì Nash-Swarm: 28.81 ms (¬±0.71)
[5/5] Analysis...

================================================================================
üìä SUMMARY: SPEED ACROSS MODEL SCALES
================================================================================

Model                Baseline     Nash-Swarm   Quant Time   Overhead  
--------------------------------------------------------------------------------
gpt2                      18.24ms      15.46ms       2.95s     0.85√ó
gpt2-medium               28.61ms      28.81ms       7.26s     1.01√ó

================================================================================
üîç KEY FINDINGS
================================================================================

1. INFERENCE SPEED:
   ‚úÖ Nash-Swarm is comparable/faster (avg 0.93√ó)
   ‚Üí Smaller quantized model = faster inference

2. QUANTIZATION TIME:
   gpt2                 2.95s (one-time setup)
   gpt2-medium          7.26s (one-time setup)
   ‚Üí Acceptable one-time cost for deployment

3. SCALING BEHAVIOR:
   Small model (gpt2): 0.85√ó
   Large model (gpt2-medium): 1.01√ó
   ‚úÖ Overhead is scale-invariant

================================================================================
üí° INTERPRETATION
================================================================================

Nash-Swarm quantization inference is FAST:
- Average overhead: 0.93√ó
- Quantized models are smaller ‚Üí faster memory access
- No runtime Nash equilibrium (only at quantization time)

IMPORTANT: This measures QUANTIZATION ONLY.
MoE routing (if used) would add separate overhead.
    
================================================================================
‚úÖ Comprehensive test complete!
================================================================================

