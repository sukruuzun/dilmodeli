
================================================================================
‚ö° SPEED BENCHMARK: BASELINE vs NASH-SWARM ‚ö°
================================================================================

[1/4] Loading gpt2...
‚úì Using MPS (Metal) acceleration

[2/4] Measuring Baseline (FP32) speed...
  Warmup (5 iterations)...
  Measuring (20 samples)...
  ‚úì Average: 17.48 ms (¬±0.84 ms)

[3/4] Quantizing model (Nash-Swarm)...
  ‚úì Quantization complete

[4/4] Measuring Nash-Swarm speed...
  Warmup (5 iterations)...
  Measuring (20 samples)...
  ‚úì Average: 16.13 ms (¬±2.20 ms)

================================================================================
üìä SPEED BENCHMARK RESULTS
================================================================================

Method               Avg Time (ms)   Std (ms)     Throughput (inf/s)
--------------------------------------------------------------------------------
Baseline (FP32)              17.48        0.84          57.20
Nash-Swarm                   16.13        2.20          61.99

================================================================================
‚ö° OVERHEAD ANALYSIS
================================================================================

‚úÖ Nash-Swarm is 1.08√ó FASTER than baseline
   (Baseline: 17.48 ms, Nash: 16.13 ms)

üìù INTERPRETATION:
   ‚úÖ EXCELLENT: Nash-Swarm is faster! (unexpected but great)

üí° NOTE:
   This measures INFERENCE ONLY (quantization already done)
   Quantization itself adds one-time setup cost
   Results may vary with different models/hardware

================================================================================
‚úÖ Benchmark complete!
================================================================================

